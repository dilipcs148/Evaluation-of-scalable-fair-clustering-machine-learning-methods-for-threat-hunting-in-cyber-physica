{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import os\n",
    "import pandas as pd\n",
    "contents = []\n",
    "dir = '/Users/sanaz/Data/UoG/Courses/Fall2019/ML/IoT/malware'\n",
    "try: \n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open( os.path.join(dir,file ) ,\"r\") as fd:\n",
    "                print fd.name\n",
    "                print fd.sort_values(by=name)\n",
    "                str1 = fd.read()\n",
    "                contents.append(str1[37:])\n",
    "    print(\"Done!\")\n",
    "except:\n",
    "    print(fd.name)\n",
    "    \n",
    "print (contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(contents)\n",
    "\n",
    "\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "nup = df.to_numpy()\n",
    "\n",
    "header = \"\"\n",
    "\n",
    "first = True\n",
    "for item in tfidf_vectorizer.get_feature_names():\n",
    "    if not first:\n",
    "        header = header + \",\"\n",
    "    else:\n",
    "        first = False\n",
    "    header = header + item\n",
    "\n",
    "header = header + \"\\n\"\n",
    "for item in tfidf_vectorizer_vectors:\n",
    "    first_vector_tfidfvectorizer = item\n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "    df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    nup = df.to_numpy()\n",
    "    first = True\n",
    "    \n",
    "    for item2 in nup:\n",
    "        if not first:\n",
    "            header = header + \",\"\n",
    "        else:\n",
    "            first = False\n",
    "        header = header + str(item2[0])\n",
    "    header = header + \",1\"    \n",
    "    header = header + \"\\n\"\n",
    "\n",
    "outfile = open('Dataset.csv', 'w')\n",
    "outfile.write(header)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "contents = []\n",
    "dir = '/Users/sanaz/Data/UoG/Courses/Fall2019/ML/IoT/goodware'\n",
    "try: \n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open( os.path.join(dir,file ) ,\"r\") as fd:\n",
    "                #print fd.name\n",
    "                #print fd.sort_values(by=name)\n",
    "                str1 = fd.read()\n",
    "                contents.append(str1[37:])\n",
    "    print(\"Done!\")\n",
    "except:\n",
    "    print(fd.name)\n",
    "    \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(contents)\n",
    "\n",
    "\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "nup = df.to_numpy()\n",
    "\n",
    "header = \"\"\n",
    "\n",
    "first = True\n",
    "for item in tfidf_vectorizer.get_feature_names():\n",
    "    if not first:\n",
    "        header = header + \",\"\n",
    "    else:\n",
    "        first = False\n",
    "    header = header + item\n",
    "\n",
    "header = header + \"\\n\"\n",
    "for item in tfidf_vectorizer_vectors:\n",
    "    first_vector_tfidfvectorizer = item\n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "    df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    nup = df.to_numpy()\n",
    "    first = True\n",
    "    \n",
    "    for item2 in nup:\n",
    "        if not first:\n",
    "            header = header + \",\"\n",
    "        else:\n",
    "            first = False\n",
    "        header = header + str(item2[0])\n",
    "    header = header + \",0\"    \n",
    "    header = header + \"\\n\"\n",
    "\n",
    "outfile = open('DatasetG.csv', 'w')\n",
    "outfile.write(header)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "\n",
    "#import csv files from folder\n",
    "path = r'/Users/sanaz/Data/UoG/Courses/Fall2019/ML'\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "with open('Outputfile.csv', 'wb') as outfile:\n",
    "    for i, fname in enumerate(allFiles):\n",
    "        with open(fname, 'rb') as infile:\n",
    "            if i != 0:\n",
    "                infile.readline()  # Throw away header on all but first file\n",
    "            # Block copy rest of file from input to output without parsing\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "            print(fname + \" has been imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9803921568627451\n",
      "[[26  1]\n",
      " [ 0 24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98        27\n",
      "           1       0.96      1.00      0.98        24\n",
      "\n",
      "    accuracy                           0.98        51\n",
      "   macro avg       0.98      0.98      0.98        51\n",
      "weighted avg       0.98      0.98      0.98        51\n",
      "\n",
      "results for Naive Bayes ----------------------------\n",
      "0.9607843137254902\n",
      "[[27  0]\n",
      " [ 2 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        27\n",
      "           1       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.96        51\n",
      "   macro avg       0.97      0.96      0.96        51\n",
      "weighted avg       0.96      0.96      0.96        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM and Naive Bayes \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "df = pd.read_csv ('combined_csv.csv')\n",
    "\n",
    "data = np.array(df.iloc[:,0:234].values)\n",
    "label = np.array(df.iloc[:,235].values)\n",
    "#Training\n",
    "svclassifier = SVC(kernel='linear')\n",
    "gnb = GaussianNB()\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "for train_index, test_index in cv.split(data):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = data[train_index], data[test_index], label[train_index], label[test_index]\n",
    "    \n",
    "    #train the model\n",
    "    svclassifier.fit(x_train, y_train)\n",
    "    gnb.fit(x_train, y_train)\n",
    "\n",
    "#Prediction\n",
    "y_pred = svclassifier.predict(x_test)\n",
    "\n",
    "#Evaluation\n",
    "print (accuracy_score(y_test, y_pred, 4))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print(\"results for Naive Bayes ----------------------------\")\n",
    "\n",
    "\n",
    "y_pred = gnb.predict(x_test)\n",
    "\n",
    "print (accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train dataset shape: ', (409, 234))\n",
      "('test dataset shape: ', (103, 234))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=1)\n",
    "#print('whole dataset shape:', X.shape)\n",
    "print('train dataset shape: ', X_train.shape)\n",
    "print('test dataset shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 256)\n",
      "Train on 460 samples, validate on 52 samples\n",
      "Epoch 1/10\n",
      "460/460 [==============================] - 1s 1ms/step - loss: 0.5470 - accuracy: 0.8391 - val_loss: 0.2771 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "460/460 [==============================] - 0s 815us/step - loss: 0.1443 - accuracy: 0.9978 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "460/460 [==============================] - 0s 757us/step - loss: 0.0293 - accuracy: 0.9957 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "460/460 [==============================] - 0s 783us/step - loss: 0.0188 - accuracy: 0.9957 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "460/460 [==============================] - 0s 794us/step - loss: 0.0139 - accuracy: 0.9978 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "460/460 [==============================] - 0s 790us/step - loss: 0.0095 - accuracy: 0.9978 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "460/460 [==============================] - 0s 794us/step - loss: 0.0082 - accuracy: 0.9978 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "460/460 [==============================] - 0s 795us/step - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "460/460 [==============================] - 0s 800us/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 9.5009e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "460/460 [==============================] - 0s 898us/step - loss: 0.0057 - accuracy: 0.9978 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Train on 460 samples, validate on 52 samples\n",
      "Epoch 1/10\n",
      "460/460 [==============================] - 0s 926us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "460/460 [==============================] - 0s 743us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 0.9808\n",
      "Epoch 3/10\n",
      "460/460 [==============================] - 0s 744us/step - loss: 8.7241e-04 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "460/460 [==============================] - 0s 941us/step - loss: 6.8572e-04 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "460/460 [==============================] - 0s 908us/step - loss: 5.8462e-04 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 0.9808\n",
      "Epoch 6/10\n",
      "460/460 [==============================] - 0s 826us/step - loss: 5.1845e-04 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 0.9808\n",
      "Epoch 7/10\n",
      "460/460 [==============================] - 0s 1ms/step - loss: 4.8563e-04 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "460/460 [==============================] - 0s 742us/step - loss: 4.2113e-04 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 0.9808\n",
      "Epoch 9/10\n",
      "460/460 [==============================] - 0s 724us/step - loss: 3.8683e-04 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 0.9808\n",
      "Epoch 10/10\n",
      "460/460 [==============================] - 0s 833us/step - loss: 3.5309e-04 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 0.9808\n",
      "Train on 461 samples, validate on 51 samples\n",
      "Epoch 1/10\n",
      "461/461 [==============================] - 0s 891us/step - loss: 0.0028 - accuracy: 0.9978 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "461/461 [==============================] - 0s 809us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 5.3401e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "461/461 [==============================] - 0s 891us/step - loss: 8.0568e-04 - accuracy: 1.0000 - val_loss: 8.0198e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "461/461 [==============================] - 0s 757us/step - loss: 6.1333e-04 - accuracy: 1.0000 - val_loss: 5.0924e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "461/461 [==============================] - 0s 742us/step - loss: 4.6756e-04 - accuracy: 1.0000 - val_loss: 5.7244e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "461/461 [==============================] - 0s 763us/step - loss: 4.0634e-04 - accuracy: 1.0000 - val_loss: 5.6445e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "461/461 [==============================] - 0s 762us/step - loss: 3.7336e-04 - accuracy: 1.0000 - val_loss: 3.7669e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "461/461 [==============================] - 0s 898us/step - loss: 3.4145e-04 - accuracy: 1.0000 - val_loss: 3.7638e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "461/461 [==============================] - 0s 874us/step - loss: 2.9168e-04 - accuracy: 1.0000 - val_loss: 2.8369e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "461/461 [==============================] - 0s 803us/step - loss: 2.8307e-04 - accuracy: 1.0000 - val_loss: 3.1271e-04 - val_accuracy: 1.0000\n",
      "Train on 461 samples, validate on 51 samples\n",
      "Epoch 1/10\n",
      "461/461 [==============================] - 0s 930us/step - loss: 2.6926e-04 - accuracy: 1.0000 - val_loss: 8.9803e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "461/461 [==============================] - 0s 859us/step - loss: 2.5944e-04 - accuracy: 1.0000 - val_loss: 8.3709e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "461/461 [==============================] - 0s 822us/step - loss: 2.5896e-04 - accuracy: 1.0000 - val_loss: 7.9964e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "461/461 [==============================] - 0s 779us/step - loss: 2.2155e-04 - accuracy: 1.0000 - val_loss: 7.6308e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "461/461 [==============================] - 0s 750us/step - loss: 2.4443e-04 - accuracy: 1.0000 - val_loss: 9.3224e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "461/461 [==============================] - 0s 742us/step - loss: 2.0304e-04 - accuracy: 1.0000 - val_loss: 6.5999e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "461/461 [==============================] - 0s 872us/step - loss: 1.5499e-04 - accuracy: 1.0000 - val_loss: 6.3996e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "461/461 [==============================] - 0s 960us/step - loss: 1.5109e-04 - accuracy: 1.0000 - val_loss: 6.0405e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "461/461 [==============================] - 0s 978us/step - loss: 1.4109e-04 - accuracy: 1.0000 - val_loss: 5.6413e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "461/461 [==============================] - 0s 770us/step - loss: 1.3345e-04 - accuracy: 1.0000 - val_loss: 5.4324e-05 - val_accuracy: 1.0000\n",
      "Train on 461 samples, validate on 51 samples\n",
      "Epoch 1/10\n",
      "461/461 [==============================] - 0s 991us/step - loss: 1.1475e-04 - accuracy: 1.0000 - val_loss: 1.5714e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "461/461 [==============================] - 0s 917us/step - loss: 1.1051e-04 - accuracy: 1.0000 - val_loss: 1.4962e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "461/461 [==============================] - 0s 902us/step - loss: 1.0336e-04 - accuracy: 1.0000 - val_loss: 1.4173e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "461/461 [==============================] - 0s 876us/step - loss: 9.8396e-05 - accuracy: 1.0000 - val_loss: 1.4068e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "461/461 [==============================] - 0s 852us/step - loss: 9.4490e-05 - accuracy: 1.0000 - val_loss: 1.4089e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "461/461 [==============================] - 0s 787us/step - loss: 9.0673e-05 - accuracy: 1.0000 - val_loss: 1.2022e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "461/461 [==============================] - 0s 882us/step - loss: 8.7703e-05 - accuracy: 1.0000 - val_loss: 1.2300e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "461/461 [==============================] - 0s 880us/step - loss: 8.2410e-05 - accuracy: 1.0000 - val_loss: 1.2227e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "461/461 [==============================] - 0s 776us/step - loss: 8.2703e-05 - accuracy: 1.0000 - val_loss: 1.2466e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "461/461 [==============================] - 0s 924us/step - loss: 7.7060e-05 - accuracy: 1.0000 - val_loss: 1.1249e-04 - val_accuracy: 1.0000\n",
      "Train on 461 samples, validate on 51 samples\n",
      "Epoch 1/10\n",
      "461/461 [==============================] - 0s 894us/step - loss: 7.9255e-05 - accuracy: 1.0000 - val_loss: 6.4861e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "461/461 [==============================] - 0s 852us/step - loss: 7.5416e-05 - accuracy: 1.0000 - val_loss: 6.0628e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "461/461 [==============================] - 0s 744us/step - loss: 7.3812e-05 - accuracy: 1.0000 - val_loss: 6.1001e-05 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "461/461 [==============================] - 0s 794us/step - loss: 6.9519e-05 - accuracy: 1.0000 - val_loss: 6.2092e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "461/461 [==============================] - 0s 807us/step - loss: 6.8082e-05 - accuracy: 1.0000 - val_loss: 6.3905e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "461/461 [==============================] - 0s 866us/step - loss: 6.4915e-05 - accuracy: 1.0000 - val_loss: 5.5157e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "461/461 [==============================] - 0s 868us/step - loss: 6.2793e-05 - accuracy: 1.0000 - val_loss: 5.6857e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "461/461 [==============================] - 0s 911us/step - loss: 6.0331e-05 - accuracy: 1.0000 - val_loss: 5.5356e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "461/461 [==============================] - 0s 830us/step - loss: 5.9108e-05 - accuracy: 1.0000 - val_loss: 5.8953e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "461/461 [==============================] - 0s 876us/step - loss: 5.6647e-05 - accuracy: 1.0000 - val_loss: 5.3827e-05 - val_accuracy: 1.0000\n",
      "Train on 461 samples, validate on 51 samples\n",
      "Epoch 1/10\n",
      "461/461 [==============================] - 0s 878us/step - loss: 5.2317e-05 - accuracy: 1.0000 - val_loss: 7.1519e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "461/461 [==============================] - 0s 820us/step - loss: 5.1641e-05 - accuracy: 1.0000 - val_loss: 7.0746e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "461/461 [==============================] - 0s 919us/step - loss: 4.8751e-05 - accuracy: 1.0000 - val_loss: 6.9576e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "461/461 [==============================] - 0s 900us/step - loss: 4.6786e-05 - accuracy: 1.0000 - val_loss: 6.8246e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "461/461 [==============================] - 0s 870us/step - loss: 4.5457e-05 - accuracy: 1.0000 - val_loss: 6.7295e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "461/461 [==============================] - 0s 1ms/step - loss: 4.4283e-05 - accuracy: 1.0000 - val_loss: 6.6119e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "461/461 [==============================] - 0s 791us/step - loss: 4.2541e-05 - accuracy: 1.0000 - val_loss: 6.5047e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "461/461 [==============================] - 0s 900us/step - loss: 4.1330e-05 - accuracy: 1.0000 - val_loss: 6.4283e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "461/461 [==============================] - 0s 883us/step - loss: 4.0181e-05 - accuracy: 1.0000 - val_loss: 6.3566e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "461/461 [==============================] - 0s 881us/step - loss: 3.9161e-05 - accuracy: 1.0000 - val_loss: 6.2310e-05 - val_accuracy: 1.0000\n",
      "Train on 461 samples, validate on 51 samples\n",
      "Epoch 1/10\n",
      "461/461 [==============================] - 0s 798us/step - loss: 4.2601e-05 - accuracy: 1.0000 - val_loss: 2.0133e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "461/461 [==============================] - 0s 766us/step - loss: 4.1359e-05 - accuracy: 1.0000 - val_loss: 1.8784e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "461/461 [==============================] - 0s 842us/step - loss: 4.0320e-05 - accuracy: 1.0000 - val_loss: 1.8085e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "461/461 [==============================] - 0s 757us/step - loss: 3.8938e-05 - accuracy: 1.0000 - val_loss: 1.8525e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "461/461 [==============================] - 0s 769us/step - loss: 3.8172e-05 - accuracy: 1.0000 - val_loss: 1.8235e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "461/461 [==============================] - 0s 746us/step - loss: 3.7032e-05 - accuracy: 1.0000 - val_loss: 1.8298e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "461/461 [==============================] - 0s 781us/step - loss: 3.5862e-05 - accuracy: 1.0000 - val_loss: 1.6728e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "461/461 [==============================] - 0s 756us/step - loss: 3.4732e-05 - accuracy: 1.0000 - val_loss: 1.6363e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "461/461 [==============================] - 0s 768us/step - loss: 3.3865e-05 - accuracy: 1.0000 - val_loss: 1.6026e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "461/461 [==============================] - 0s 749us/step - loss: 3.3123e-05 - accuracy: 1.0000 - val_loss: 1.5984e-05 - val_accuracy: 1.0000\n",
      "Train on 461 samples, validate on 51 samples\n",
      "Epoch 1/10\n",
      "461/461 [==============================] - 0s 874us/step - loss: 3.3305e-05 - accuracy: 1.0000 - val_loss: 1.3230e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "461/461 [==============================] - 0s 770us/step - loss: 3.3022e-05 - accuracy: 1.0000 - val_loss: 1.6725e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "461/461 [==============================] - 0s 775us/step - loss: 3.1527e-05 - accuracy: 1.0000 - val_loss: 1.4812e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "461/461 [==============================] - 0s 876us/step - loss: 2.9977e-05 - accuracy: 1.0000 - val_loss: 1.4695e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "461/461 [==============================] - 0s 802us/step - loss: 2.8821e-05 - accuracy: 1.0000 - val_loss: 1.2877e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "461/461 [==============================] - 0s 848us/step - loss: 2.8342e-05 - accuracy: 1.0000 - val_loss: 1.1784e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "461/461 [==============================] - 0s 787us/step - loss: 2.7148e-05 - accuracy: 1.0000 - val_loss: 1.3099e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "461/461 [==============================] - 0s 781us/step - loss: 2.6731e-05 - accuracy: 1.0000 - val_loss: 1.2601e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "461/461 [==============================] - 0s 896us/step - loss: 2.5914e-05 - accuracy: 1.0000 - val_loss: 1.1379e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "461/461 [==============================] - 0s 764us/step - loss: 2.4953e-05 - accuracy: 1.0000 - val_loss: 1.0519e-05 - val_accuracy: 1.0000\n",
      "Train on 461 samples, validate on 51 samples\n",
      "Epoch 1/10\n",
      "461/461 [==============================] - 0s 762us/step - loss: 2.3060e-05 - accuracy: 1.0000 - val_loss: 2.2801e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "461/461 [==============================] - 0s 842us/step - loss: 2.2680e-05 - accuracy: 1.0000 - val_loss: 2.3556e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "461/461 [==============================] - 0s 874us/step - loss: 2.2107e-05 - accuracy: 1.0000 - val_loss: 2.2656e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "461/461 [==============================] - 0s 904us/step - loss: 2.1598e-05 - accuracy: 1.0000 - val_loss: 2.2591e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "461/461 [==============================] - 0s 844us/step - loss: 2.1201e-05 - accuracy: 1.0000 - val_loss: 2.2388e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "461/461 [==============================] - 0s 779us/step - loss: 2.0784e-05 - accuracy: 1.0000 - val_loss: 2.2420e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "461/461 [==============================] - 0s 820us/step - loss: 2.0332e-05 - accuracy: 1.0000 - val_loss: 2.3378e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "461/461 [==============================] - 0s 870us/step - loss: 1.9709e-05 - accuracy: 1.0000 - val_loss: 2.1514e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "461/461 [==============================] - 0s 935us/step - loss: 1.9316e-05 - accuracy: 1.0000 - val_loss: 2.1392e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "461/461 [==============================] - 0s 904us/step - loss: 1.8930e-05 - accuracy: 1.0000 - val_loss: 1.8695e-05 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#CNN Implementation\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "df = pd.read_csv ('combined_csv.csv')\n",
    "\n",
    "num_classes = 2\n",
    "img_rows, img_cols = 16, 16\n",
    "\n",
    "data = np.array(df.iloc[:,0:234].values)\n",
    "\n",
    "#Creating zero matrix and concatenate \n",
    "zero_matrix = np.zeros((data.shape[0],22))\n",
    "data = np.concatenate((data, zero_matrix), axis=1)\n",
    "for i in range(0, (data.shape[1])):\n",
    "    data[:, i] = sklearn.preprocessing.minmax_scale(data[:, i], feature_range=(0, 1), axis=0, copy=True)\n",
    "    \n",
    "label = np.array(df.iloc[:,235].values)\n",
    "data, label = shuffle(data, label)\n",
    "label = keras.utils.to_categorical(label, num_classes)\n",
    "\n",
    "print (data.shape)\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(16, 16, 1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "for train_index, test_index in cv.split(data):\n",
    "    #print(\"dataset name: \", item, \"and \",i,\"th Fold\" )\n",
    "    x_train, x_test, y_train, y_test = data[train_index], data[test_index], label[train_index], label[test_index]\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    #train the model\n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/54/5f/e1b2d83b808f978f51b7ce109315154da3a3d4151aa59686002681f2e109/tensorflow-2.0.0-cp37-cp37m-win_amd64.whl (48.1MB)\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl (57kB)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/94/1d/82d9b8028f1e85bbe9beb74058b276db9cfb7e1afdb3174651a4828e5e9e/protobuf-3.11.1-cp37-cp37m-win_amd64.whl (1.0MB)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting gast==0.2.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.4)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.4)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/a8/91d878054bdacf586ed613bec64f5c26cc0a0a2af294377c7f29469a05f4/grpcio-1.25.0-cp37-cp37m-win_amd64.whl (1.8MB)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.22.0)\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/54/31/f944cbd5bdbcc90d5b36f0615036308c8ec1e41b4788da5b55d4900f6803/google_auth-1.8.2-py2.py3-none-any.whl (75kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.15.4)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Requirement already satisfied: h5py in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\snakh\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.6.16)\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "Collecting cachetools<3.2,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Building wheels for collected packages: absl-py, termcolor, gast, opt-einsum\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\snakh\\AppData\\Local\\pip\\Cache\\wheels\\a7\\15\\a0\\0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\snakh\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\snakh\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for opt-einsum (setup.py): started\n",
      "  Building wheel for opt-einsum (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\snakh\\AppData\\Local\\pip\\Cache\\wheels\\2c\\b1\\94\\43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n",
      "Successfully built absl-py termcolor gast opt-einsum\n",
      "Installing collected packages: google-pasta, protobuf, absl-py, termcolor, astor, gast, grpcio, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard, opt-einsum, tensorflow-estimator, tensorflow\n",
      "Successfully installed absl-py-0.8.1 astor-0.8.1 cachetools-3.1.1 gast-0.2.2 google-auth-1.8.2 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.25.0 markdown-3.1.1 oauthlib-3.1.0 opt-einsum-3.1.0 protobuf-3.11.1 pyasn1-0.4.8 pyasn1-modules-0.2.7 requests-oauthlib-1.3.0 rsa-4.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "kmeans.fit(X_train, y_train)\n",
    "y_kmeans = kmeans.predict(X_test)\n",
    "\n",
    "#print(X_test.shape)\n",
    "#print(y_kmeans.shape)\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.scatter(X_test[:, 0], X_test[:, 1], c=y_kmeans, s=50, cmap='viridis');\n",
    "\n",
    "#Evaluation\n",
    "print (accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_kmeans))\n",
    "print(classification_report(y_test,y_kmeans))\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_kmeans)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "#++++++++++++++++++++++++++++++++++++\n",
    "tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "#++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes = ['Goodware', 'Malware'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, normalize=True, classes = ['Goodware', 'Malware'],\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"results for Spectral: --------------------------------------------\")\n",
    "# Spectral Clustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "spectralmodel = SpectralClustering(n_clusters=2)\n",
    "spectralmodel.fit(X_train, y_train)\n",
    "y_spectralmodel = spectralmodel.fit_predict(X_test)\n",
    "\n",
    "#Evaluation\n",
    "print (accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_spectralmodel))\n",
    "print(classification_report(y_test,y_spectralmodel))\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_spectralmodel)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "#++++++++++++++++++++++++++++++++++++\n",
    "tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "#++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes = ['Goodware', 'Malware'], title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, normalize=True, classes = ['Goodware', 'Malware'], title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN implementation\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_knn = knn.predict(X_test)\n",
    "\n",
    "#Evaluation\n",
    "print (accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_knn))\n",
    "print(classification_report(y_test,y_knn))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_knn)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes = ['Goodware', 'Malware'], title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, normalize=True, classes = ['Goodware', 'Malware'], title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
